
\begin{abstract}
复杂电磁环境下的无线信号认知是优化频谱利用效率、识别和最小化干扰的重要方法。
传统的调制识别方法主要集中在能量检测以及专家特征和决策准则的使用。
随着深度学习发展，利用原始采样信号学习信号特征并进行识别调制方式得以实现，
并且在低信噪比条件下的准确率优于传统的基于专家特征提取的统计机器学习模型。
本文对基于深度学习的调制识别算法、特征融合框架、影响模型性能的因素等进行了研究，
主要贡献如下：\par

1. 本文提出了一种CAE-CNN调制识别的算法框架，并提出了相应的训练算法。
我们分别利用监督方法和无监督方法将信号特征可视化，
发现通过无监督的卷积自编码器可以复现输入信号，获得的数据低维表示具有一定的类别区分性；
通过有监督的CNN获取的信号低维表示，对不同类别的调制信号具有较强的区分度，利用t-SNE算法
降到二维的流型中，相同类别的信号独立成一个簇。
我们融合卷积自编码器与卷积神经网络，提出了CAE-CNN算法框架和相应训练算法。
仿真结果显示，我们提出的算法，在高信噪比条件下具有较高的识别准确率（95\%以上），并且鲁棒性较强；
在低信噪比条件下具备比传统方法更高的识别率，在SNR为-4dB时准确率仍在85\%左右。
在算法效率上，我们提出的算法训练时间在可接受范围以内，同时算法分类识别时间也较低。\par

2. 提出了一种传统特征与深度特征融合的框架。
将传统方法中常用的调制识别的特征与CNN获取的深度特征进行适应性的$Batch-Normalization$，
然后分别针对Softmax、随机森林、深度神经网络等融合算法构建融合模型，
并对不同融合算法的进行了仿真仿真。
仿真结果显示，基于随机森林和基于Softmax的融合算法性能由于基准的卷积神经网络，
并且基于RF的融合算法具有相对最优的分类性能，无论在低信噪比还是高信噪比都具有较强的鲁棒性。
同时，受融合算法特性等的影响，不同融合框架下的误分信号各不相同。 \par

3. 我们从网络底层研究网络超参数对调制识别性能的影响，
并从欠拟合与过拟合以及偏差与方差的角度解释了仿真结果。
CNN在调制识别时的性能似乎不受网络深度的限制，在卷积层数目为$3$时即可达到性能的最值，
此时再增大卷积层的深度，系统性能可能会发生下降。
第一层卷积核数目较小时，网络的性能几乎随着第二层卷积核数目的增大而减小。
而随着第一层卷积核增大，改变第二层卷积核数目，系统分类准确率变化很小，只有轻微的波动。
整体而言，卷积核宽度$W^{(1)}$和$W^{(2)}$较大的网络性能较好，
但是当卷积的宽度增大到$7$之后，分类性能较为稳定，仅仅发生微小的波动。\par

\keywords{调制识别,\quad{}深度学习,\quad{}网络超参数,\quad{}特征融合} \\
\end{abstract}

\begin{englishabstract}
Wireless signal recognition in complex electromagnetic environments is an important method for optimizing spectrum utilization efficiency, identifying and minimizing interference. Traditional modulation identification methods mainly focus on energy detection and the use of expert characteristics and decision criteria. With the development of deep learning, the original sampled signal is used to learn the signal features and identify and modulate the signal. The accuracy under low signal-to-noise ratio is better than the traditional statistical machine learning model based on expert feature extraction. This paper studies the modulation recognition algorithms based on deep learning, feature fusion framework, and factors that affect the performance of the model. The main contributions are as follows:
\par~\par
1. This paper proposes a CAE-CNN modulation recognition algorithm framework and proposes a corresponding training algorithm. We visualized the signal characteristics using supervised methods and unsupervised methods, respectively, and found that the unsupervised convolutional self-encoder can reproduce the input signals. The low-dimensional representation of the obtained data has a certain class distinction; it is obtained through supervised CNN. The low dimension of the signal indicates that there is a strong degree of discrimination between the different classes of modulated signals, which is reduced to a two-dimensional flow pattern using the t-SNE algorithm, and the signals of the same class form a cluster independently. We fuse convolutional self-encoders and convolutional neural networks and propose a CAE-CNN algorithm framework and corresponding training algorithms. The simulation results show that the proposed algorithm has higher recognition accuracy (95\% or more) under high signal-to-noise ratio, and has stronger robustness; it has higher performance than traditional methods under low signal-to-noise ratio conditions. The recognition rate is still about 85\% when the SNR is -4dB. In terms of algorithm efficiency, the training time of our proposed algorithm is within an acceptable range, and the classification and recognition time of the algorithm is also low.
\par~\par
2. A framework for the fusion of traditional features and depth features is proposed. The $Batch-Normalization$ that adapts the features of modulation recognition commonly used in traditional methods and the depth features obtained by CNN, and then construct a fusion model for fusion algorithms such as Softmax, random forest, and deep neural network, and the fusion algorithms for different fusion algorithms Simulations were performed. The simulation results show that the performance of the fusion algorithm based on random forest and Softmax is due to the benchmark convolutional neural network, and the RF-based fusion algorithm has relatively optimal classification performance, which is comparable in both low SNR and high SNR. Strong robustness. At the same time, due to the influence of the characteristics of the fusion algorithm, the misclassification signals under different fusion frameworks are different.
\par~\par
3. We study the effect of network hyperparameters on modulation recognition performance from the network bottom and explain the simulation results from the perspective of under-fitting and over-fitting, as well as deviation and variance. The performance of CNN in modulation recognition does not seem to be limited by the depth of the network. The maximum value of performance can be achieved when the number of convolution layers is $3$. At this time, the depth of the convolutional layer is further increased, and the system performance may decrease. When the number of first-level convolutional cores is small, the performance of the network decreases almost as the number of second-level convolutional cores increases. With the increase of the number of convolutional nuclei at the first level, the number of second-level convolutional nuclei changes, and the accuracy of system classification changes only slightly, with only slight fluctuations. Overall, the larger network performance of the convolution kernel widths $W^{(1)}$ and $W^{(2)}$ is better, but the classification performance is increased when the convolution width is increased to $7$. It is relatively stable and only minor fluctuations occur.
~\par
\englishkeywords{Modulation identification,\space{}deep Learning,\space{}network hyper-parameter,\space{}feature fusion}

\end{englishabstract}



\XDUpremainmatter

\begin{symbollist}
	\item \makebox[14em][l]{~符号} \makebox[18em][l]{~~符号名称}  
	\item \makebox[15em][l]{$\in$}  \makebox[18em][l]{属于}  	
	\item \makebox[15em][l]{$x$}  \makebox[18em][l]{样本}  
	\item \makebox[15em][l]{$y$}  \makebox[18em][l]{标签}  
	\item \makebox[15em][l]{$N$}  \makebox[18em][l]{样本数量}  
	\item \makebox[15em][l]{$\eta$}  \makebox[18em][l]{学习率}  
	\item \makebox[15em][l]{$*$}  \makebox[18em][l]{卷积}  
	\item \makebox[15em][l]{$w$}  \makebox[18em][l]{神经网络权重}  
	\item \makebox[15em][l]{$b$}  \makebox[18em][l]{神经网络的偏置}  
	\item \makebox[15em][l]{$\lambda$}  \makebox[18em][l]{权重衰减系数}  
	\item \makebox[15em][l]{$\Delta$}  \makebox[18em][l]{求导}  
	\item \makebox[15em][l]{$\min$}  \makebox[18em][l]{求最小}  
	\item \makebox[15em][l]{$\max$}  \makebox[18em][l]{求最大}  
	\item \makebox[15em][l]{$\alpha$}  \makebox[18em][l]{学习速率衰减因子}  
	\item \makebox[15em][l]{$\theta$}  \makebox[18em][l]{网络参数}  
	\item \makebox[15em][l]{$J(\cdot)$}  \makebox[18em][l]{损失函数}  
	\item \makebox[15em][l]{$I$}  \makebox[18em][l]{指示函数}  
	\item \makebox[15em][l]{$E$}  \makebox[18em][l]{期望}  
	\item \makebox[15em][l]{$bias$}  \makebox[18em][l]{偏差}  
	\item \makebox[15em][l]{$\delta^2$}  \makebox[18em][l]{方差}  
	\item \makebox[15em][l]{$O$}  \makebox[18em][l]{复杂度}  
	\item \makebox[15em][l]{$T$}  \makebox[18em][l]{学习器}  

\end{symbollist}

\begin{abbreviationlist}
	\item \makebox[6em][l] {Adam}  \makebox[20em][l]{Adaptive Moment Estimation}    \makebox[16em][l]{自适应矩估计}
	\item \makebox[6em][l]{AE}  \makebox[20em][l]{autoencoder}    \makebox[16em][l]{自动编码器}
	\item \makebox[6em][l]{BN}  \makebox[20em][l]{Batch Normalization}    \makebox[16em][l]{批归一化}
	\item \makebox[6em][l]{CAE}  \makebox[20em][l]{Convolutional Auto-Encode}    \makebox[16em][l]{卷积自编码器}
	\item \makebox[6em][l]{CART}  \makebox[20em][l]{Classification and Regression Tree}    \makebox[16em][l]{CART树}
	\item \makebox[6em][l]{CNN}  \makebox[20em][l]{Convolutional Neural Network}    \makebox[16em][l]{卷积神经网络}
	\item \makebox[6em][l]{DBN}  \makebox[20em][l]{Deep Belief Network}    \makebox[16em][l]{深度置信网络}
	\item \makebox[6em][l]{DL}  \makebox[20em][l]{Deep Learning}    \makebox[16em][l]{深度学习}
	\item \makebox[6em][l]{DSA}  \makebox[20em][l]{Dynamic Spectrum Access}    \makebox[16em][l]{动态频谱接入}
	\item \makebox[6em][l]{DTree}  \makebox[20em][l]{Decision Tree}    \makebox[16em][l]{决策树}
	\item \makebox[6em][l]{EL}  \makebox[20em][l]{Ensemble Learning}    \makebox[16em][l]{集成学习} 
	\item \makebox[6em][l]{GAN}  \makebox[20em][l]{Generative Adversarial Networks}    \makebox[16em][l]{生成对抗网络}
	\item \makebox[6em][l]{IF}  \makebox[20em][l]{Indicator Function}    \makebox[16em][l]{指示函数}
	\item \makebox[6em][l]{KNN}  \makebox[20em][l]{K-Nearest Neighbor}    \makebox[16em][l]{K-近邻}
	\item \makebox[6em][l]{LR}  \makebox[20em][l]{Logistic Regression}    \makebox[16em][l]{逻辑回归}
	\item \makebox[6em][l]{ML}  \makebox[20em][l]{Machine Learning}    \makebox[16em][l]{机器学习}
	\item \makebox[6em][l]{MSE}  \makebox[20em][l]{Mean Squared Error}    \makebox[16em][l]{均方误差}
	\item \makebox[6em][l]{NN}  \makebox[20em][l]{Neural Networks}    \makebox[16em][l]{神经网络}
	\item \makebox[6em][l]{PCA}  \makebox[20em][l]{Principal components analysis}    \makebox[16em][l]{主成分分析}
	\item \makebox[6em][l]{RF}  \makebox[20em][l]{Random Forest}    \makebox[16em][l]{随机森林}
	\item \makebox[6em][l]{SAE}  \makebox[20em][l]{Sparse Autoencoder}    \makebox[16em][l]{稀疏自编码器}
	\item \makebox[6em][l]{SGD}  \makebox[20em][l]{Stochastic Gradient Descent}    \makebox[16em][l]{统计梯度下降}
	\item \makebox[6em][l]{SR}  \makebox[20em][l]{Softmax Regression}    \makebox[16em][l]{Softmax回归}
	\item \makebox[6em][l]{SVM}  \makebox[20em][l]{Support Vector Machine}    \makebox[16em][l]{支持向量机}
	\item \makebox[6em][l]{t-SNE}  \makebox[20em][l]{t-Distributed Stochastic Neighbor Embedding}    \makebox[16em][l]{t-分布随机邻接嵌入}
\end{abbreviationlist}
